Goal: Create a full-stack application that recursively crawls a website's sitemap structure, extracts all associated URLs, and tracks the exact sitemap path (source) for each discovered URL.

1. Project Title & Stack
Project Title: Recursive Sitemap Analyzer & Source Tracker

Technology Stack:

Frontend: Next.js (with TypeScript/React)

Backend: Node.js (Express preferred for REST API)

Database/Storage: Simple in-memory storage (for MVP) or a file/database (e.g., SQLite, PostgreSQL) for persistent storage of the final URL list.

2. Frontend (Next.js) Requirements
Input Interface: A clean, responsive form field for the user to input the Entry Sitemap URL (e.g., https://example.com/sitemap-index.xml).

Action Trigger: A button labeled "Start Analysis" that remains disabled until a valid-looking URL is provided.

Status Display: A live status or log area to show the crawling progress (e.g., "Fetching: rootSiteMap.xml," "Found 50 URLs," "Processing 2 nested sitemaps...").

Output Display: A dedicated section to display the final global list of URLs in a sortable/filterable table format. The table must clearly show the two required fields: url and source.

3. Backend (Node.js/Express) Requirements
Endpoint: Create a primary REST API endpoint (e.g., POST /api/crawl) that accepts the Entry Sitemap URL.

Core Logic: The backend must implement a recursive sitemap parsing function to handle both standard <url> sitemaps and <sitemap> index files.

Sitemap Fetching: Use an appropriate library (e.g., axios, fetch) to retrieve XML content and an XML parsing library to read the tags.

Recursion: If the fetched document is a sitemap index (contains <sitemap> tags), the function must call itself for each nested sitemap URL.

Rate Limiting & Politeness: Implement basic politeness: add a delay (e.g., 500ms) between subsequent fetches to avoid overwhelming the target server.

4. Data Structure and Source Tracking (Crucial)
The core requirement is to accurately track the source path for every URL:

Global List Structure: Maintain a single global list of discovered URLs. Each entry in this list must be an object with the following structure:

JSON

{
  "url": "string",
  "source": "string"
}
Source Path Logic: The source field must represent the complete hierarchical path from the initial entry sitemap:

Level 1 (Direct URL): If a URL is found in the entry sitemap, source is simply the entry sitemap's filename (e.g., "rootSiteMap.xml").

Nested URL: If a URL is found in a nested sitemap, the source must trace the path back to the root using the => delimiter: "rootSiteMap.xml=>childSiteMap.xml=>grandchildSiteMap.xml".

5. Constraints and Error Handling
Duplication: Only unique URLs should be added to the global list. If a URL is found in multiple sitemaps, the first encountered source path should be retained.

Error Handling: The system must gracefully handle HTTP errors (e.g., 404, 500) and XML parsing errors without crashing the entire crawl process. Any failed sitemap URL should be logged but the crawl should continue with other sitemaps.